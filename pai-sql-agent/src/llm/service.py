"""
í†µí•© LLM ì„œë¹„ìŠ¤ - ëª¨ë“  LLM ê´€ë ¨ ê¸°ëŠ¥ì„ ë‹´ë‹¹
í†µê³„ì²­ ë° SGIS ë°ì´í„° ë¶„ì„ìš© LLM ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, AsyncGenerator
from dataclasses import dataclass

from pydantic_settings import BaseSettings
from pydantic import Field
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage

from src.agent.settings import get_settings

logger = logging.getLogger(__name__)


class LLMConfig(BaseSettings):
    """
    LLM ì„¤ì • (pydantic_settings ê¸°ë°˜)
    
    í†µê³„ì²­ ë° SGIS: LLM ëª¨ë¸ ì„¤ì • ê´€ë¦¬ìš© llm ì„œë¹„ìŠ¤ ì „ìš©
    í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°’ì„ ì½ì–´ì˜¤ë©°, ê¸°ë³¸ê°’ ì œê³µ
    """
    
    # ëª¨ë¸ ì„¤ì •
    provider: str = Field(
        default="google",  # ê¸°ë³¸: Google Gemini, ì „í™˜ì‹œ: "openai"
        description="LLM í”„ë¡œë°”ì´ë” (openai, google)"
    )
    model_name: str = Field(
        default="gemini-2.5-flash-lite",  # Google ê¸°ë³¸ê°’, OpenAI: "gpt-4o-mini"
        description="ì‚¬ìš©í•  LLM ëª¨ë¸ëª… (gemini-2.5-flash-lite, gpt-4o-mini)"
    )
    temperature: float = Field(
        default=0.1,
        ge=0.0,
        le=2.0,
        description="LLM ì°½ì˜ì„± ìˆ˜ì¤€ (0.0-2.0)"
    )
    max_tokens: int = Field(
        default=2000,
        gt=0,
        description="ìµœëŒ€ í† í° ìˆ˜"
    )
    streaming: bool = Field(
        default=True,
        description="ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ í™œì„±í™”"
    )
    timeout: int = Field(
        default=60,
        gt=0,
        description="ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)"
    )
    
    # ì„±ëŠ¥ ì„¤ì •
    max_retries: int = Field(
        default=3,
        ge=0,
        description="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜"
    )
    request_timeout: float = Field(
        default=30.0,
        gt=0.0,
        description="ê°œë³„ ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)"
    )
    
    # ê³ ê¸‰ ì„¤ì •
    top_p: float = Field(
        default=0.9,
        ge=0.0,
        le=1.0,
        description="Top-p ìƒ˜í”Œë§ ê°’"
    )
    frequency_penalty: float = Field(
        default=0.0,
        ge=-2.0,
        le=2.0,
        description="ë¹ˆë„ íŒ¨ë„í‹°"
    )
    presence_penalty: float = Field(
        default=0.0,
        ge=-2.0,
        le=2.0,
        description="ì¡´ì¬ íŒ¨ë„í‹°"
    )
    
    class Config:
        env_prefix = "LLM_"  # í™˜ê²½ë³€ìˆ˜ ì ‘ë‘ì‚¬
        case_sensitive = False


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ ë°ì´í„°"""
    content: str
    model: str
    usage: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None


class LLMService:
    """
    í†µí•© LLM ì„œë¹„ìŠ¤
    
    ì—­í• :
    - LLM ëª¨ë¸ ê´€ë¦¬
    - í†µê³„ ë°ì´í„° ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
    - ìŠ¤íŠ¸ë¦¬ë° ë° ì¼ë°˜ ì‘ë‹µ ì§€ì›
    - ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
    """
    
    def __init__(self, config: Optional[LLMConfig] = None):
        """
        LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            config: LLM ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        """
        self.config = config or LLMConfig()
        self.settings = get_settings()
        self._llm: Optional[BaseChatModel] = None
        
        logger.info(f"LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {self.config.model_name}")
    
    @property
    def llm(self) -> BaseChatModel:
        """LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì§€ì—° ì´ˆê¸°í™”)"""
        if self._llm is None:
            self._llm = self._create_llm()
        return self._llm
    
    def _create_llm(self) -> BaseChatModel:
        """LLM ëª¨ë¸ ìƒì„±"""
        try:
            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            provider = self.config.provider.lower().strip() if self.config.provider else "google"
            model_name = self.config.model_name.strip() if self.config.model_name else "gemini-2.5-flash-lite"
            
            logger.info(f"ğŸ”§ ì‚¬ìš©í•  Provider: '{provider}', Model: '{model_name}'")
            
            if provider == "google":
                # Google Gemini ëª¨ë¸ ìƒì„±
                llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    timeout=self.config.request_timeout,
                    max_retries=self.config.max_retries,
                    top_p=self.config.top_p,
                    google_api_key=self.settings.google_api_key,
                )
                logger.info(f"Google Gemini ëª¨ë¸ ìƒì„±: {model_name}")
                
            elif provider == "openai":
                # OpenAI GPT ëª¨ë¸ ìƒì„±
                llm = ChatOpenAI(
                    model=model_name,
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    streaming=self.config.streaming,
                    timeout=self.config.request_timeout,
                    max_retries=self.config.max_retries,
                    top_p=self.config.top_p,
                    frequency_penalty=self.config.frequency_penalty,
                    presence_penalty=self.config.presence_penalty,
                    openai_api_key=self.settings.openai_api_key,
                )
                logger.info(f"OpenAI GPT ëª¨ë¸ ìƒì„±: {model_name}")
                
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ Google Gemini ì‚¬ìš©
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í”„ë¡œë°”ì´ë” '{provider}', Google Geminië¡œ ê¸°ë³¸ ì„¤ì •")
                llm = ChatGoogleGenerativeAI(
                    model="gemini-2.5-flash-lite",
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    timeout=self.config.request_timeout,
                    max_retries=self.config.max_retries,
                    top_p=self.config.top_p,
                    google_api_key=self.settings.google_api_key,
                )
                logger.info(f"ê¸°ë³¸ Google Gemini ëª¨ë¸ ìƒì„±: gemini-2.5-flash-lite")
            
            logger.info(f"LLM ëª¨ë¸ ìƒì„± ì™„ë£Œ: {provider}/{model_name}")
            return llm
            
        except Exception as e:
            logger.error(f"LLM ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def generate(
        self,
        messages: List[BaseMessage],
        **kwargs
    ) -> LLMResponse:
        """LLM ì‘ë‹µ ìƒì„± (ë¹„ìŠ¤íŠ¸ë¦¬ë°)"""
        try:
            # ìˆ˜ì •: streaming íŒŒë¼ë¯¸í„° ì œê±°
            response = await self.llm.ainvoke(messages, **kwargs)
            
            return LLMResponse(
                content=response.content,
                model=self.config.model_name,
                usage=getattr(response, 'usage_metadata', None),
                metadata=getattr(response, 'response_metadata', None)
            )
            
        except Exception as e:
            logger.error(f"LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    async def generate_stream(
        self,
        messages: List[BaseMessage],
        **kwargs
    ):
        """
        LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            **kwargs: ì¶”ê°€ LLM íŒŒë¼ë¯¸í„°
        
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        try:
            # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            temp_llm = self.llm.bind(streaming=True, **kwargs)
            
            async for chunk in temp_llm.astream(messages):
                yield chunk
                
        except Exception as e:
            logger.error(f"LLM ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
            raise
    
    def create_human_message(self, content: str) -> HumanMessage:
        """ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±"""
        return HumanMessage(content=content)
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        return {
            "model_name": self.config.model_name,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "streaming": self.config.streaming,
            "timeout": self.config.timeout
        }
    
    async def get_model(self, config: Optional[LLMConfig] = None) -> BaseChatModel:
        """
        LLM ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        """
        if config and config != self.config:
            # ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ ì„ì‹œ ëª¨ë¸ ìƒì„±
            temp_service = LLMService(config)
            return temp_service.llm
        
        return self.llm
    
    def update_config(self, **kwargs) -> LLMConfig:
        """
        ì„¤ì • ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ ì„¤ì • ê°ì²´ ë°˜í™˜)
        """
        config_dict = self.config.model_dump()
        config_dict.update(kwargs)
        return LLMConfig(**config_dict)
    
    async def test_connection(self) -> bool:
        """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_message = HumanMessage(content="test")
            await self.llm.ainvoke([test_message])
            logger.info("LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            return True
            
        except Exception as e:
            logger.error(f"LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False


# ê¸€ë¡œë²Œ LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_llm_service: Optional[LLMService] = None
_lock = asyncio.Lock()


async def get_llm_service(config: Optional[LLMConfig] = None) -> LLMService:
    """
    LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)
    
    Args:
        config: LLM ì„¤ì • (ì²« ë²ˆì§¸ í˜¸ì¶œ ì‹œì—ë§Œ ì ìš©)
    
    Returns:
        LLM ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _llm_service
    
    if _llm_service is None:
        async with _lock:
            if _llm_service is None:
                _llm_service = LLMService(config)
    
    return _llm_service


