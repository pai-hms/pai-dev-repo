"""
LangGraph Agent ë…¸ë“œë“¤
LLM ì‘ë‹µ ìƒì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë…¸ë“œ êµ¬ì¡°

ì„¤ê³„ ì›ì¹™:
- ë°ì´í„°ì™€ ë¡œì§ì˜ ì¼ì²´í™”: ë©”ì‹œì§€ ì²˜ë¦¬ì™€ ìƒíƒœ ê´€ë¦¬ë¥¼ í•¨ê»˜ ì²˜ë¦¬
- ì„ í˜•ì›ë¦¬: ì§ì„ ì ì¸ ì²˜ë¦¬ íë¦„ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ
- SLAP: ê° í•¨ìˆ˜ëŠ” ë™ì¼í•œ ì¶”ìƒí™” ìˆ˜ì¤€ ìœ ì§€
"""
import logging
from typing import Dict, Any, TypedDict, List, Literal, Annotated, Optional
from datetime import datetime
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, AnyMessage
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.tools import BaseTool
from langgraph.graph.message import add_messages

from .prompt import get_database_schema, get_react_agent_initial_prompt, get_react_agent_response_prompt

logger = logging.getLogger(__name__)

# ===== ìƒíƒœ ì •ì˜ =====

class SupervisorInputState(TypedDict):
    """Supervisorì˜ Input"""
    query: str


class SupervisorOutputState(TypedDict):
    """Supervisorì˜ Output"""
    sql_query: str
    data: Dict[str, Dict[str, Any]]


class LangGraphAgentState(SupervisorInputState, SupervisorOutputState):
    """LangGraph Agentì˜ ì „ì²´ ìƒíƒœ (input + output + hidden)"""
    messages: Annotated[List[BaseMessage], add_messages]


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
ReactAgentState = LangGraphAgentState
AgentState = LangGraphAgentState


def create_initial_state(query: str, thread_id: str = "default") -> LangGraphAgentState:
    """ì´ˆê¸° ìƒíƒœ ìƒì„±"""
    return {
        "messages": [HumanMessage(content=query)],
        "query": query,
        "sql_query": "",
        "data": {}
    }


# í•˜ìœ„ í˜¸í™˜ì„±
create_react_initial_state = create_initial_state


# ===== ë©”ì¸ LLM ì‘ë‹µ ìƒì„± ë…¸ë“œ =====

class LangAgentNode:
    """
    LLMì„ í†µí•´ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì¸ ë…¸ë“œ
    
    ì„¤ê³„ ì›ì¹™:
    - ë°ì´í„°ì™€ ë¡œì§ì˜ ì¼ì²´í™”: ë©”ì‹œì§€ ì²˜ë¦¬, í† í° ê´€ë¦¬, ë„êµ¬ ì—°ë™ì„ í†µí•© ê´€ë¦¬
    - SLAP: ê° ë©”ì„œë“œëŠ” ë™ì¼í•œ ì¶”ìƒí™” ìˆ˜ì¤€ ìœ ì§€
    - Containerì„ í†µí•œ ì˜ì¡´ê´€ê³„ ëª…ì„¸: í•„ìš”í•œ ì„œë¹„ìŠ¤ë“¤ì„ DIë¡œ ì£¼ì…
    """

    def __init__(
        self,
        execution_service: Any,
        tools: Dict[str, BaseTool],
        prompt_generator: Any,
        token_usage_service: Any,
    ):
        """
        Args:
            execution_service: ëª¨ë¸ ì‹¤í–‰ ì„œë¹„ìŠ¤
            tools: ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤ (ì´ë¦„ -> ë„êµ¬ ë§¤í•‘)
            prompt_generator: í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
            token_usage_service: í† í° ì‚¬ìš©ëŸ‰ ì„œë¹„ìŠ¤
        """
        self.execution_service = execution_service
        self.tools = tools
        self.prompt_generator = prompt_generator
        self.token_usage_service = token_usage_service

    async def __call__(
        self, state: LangGraphAgentState, config: RunnableConfig = None
    ) -> LangGraphAgentState:
        """
        ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ - LLMì„ í†µí•œ ì‘ë‹µ ìƒì„±
        
        ì²˜ë¦¬ íë¦„:
        1. ì„¤ì •ì—ì„œ ì±—ë´‡ ì •ë³´ ì¶”ì¶œ
        2. ëª¨ë¸ ë¡œë“œ ë° ë„êµ¬ ë°”ì¸ë”©
        3. í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±
        4. SQL ë°ì´í„° ìƒíƒœ ê´€ë¦¬
        5. LLM í˜¸ì¶œ ë° ì‘ë‹µ ìƒì„±
        6. ë©”íƒ€ë°ì´í„° ì¶”ê°€ (parent_id, timestamp)
        7. í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
        """
        try:
            # 1. ì±—ë´‡ ì„¤ì • ì¶”ì¶œ
            chatbot_info = self._extract_chatbot_info(config)
            
            # 2. ëª¨ë¸ ë¡œë“œ
            model = await self._load_chatmodel(chatbot_info, config)
            
            # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt_template = await self._create_prompt_template(chatbot_info, config)
            chain = prompt_template | model
            
            # 4. ë°ì´í„° ìƒíƒœ ê´€ë¦¬
            state = self._manage_sql_data_state(state)
            
            # 5. LLM í˜¸ì¶œ
            message = await self._invoke_chain(chain, state["messages"], config)
            
            # 6. ë©”íƒ€ë°ì´í„° ì¶”ê°€
            message = self._add_message_metadata(message, state["messages"])
            state = self._add_metadata_to_existing_messages(state, message)
            
            # 7. í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
            await self._accumulate_token_usage(chatbot_info, message)
            
            return {"messages": [message], "data": state.get("data", {})}
            
        except Exception as e:
            logger.error(f"âŒ LangAgentNode ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            error_message = AIMessage(content=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return {"messages": [error_message], "data": state.get("data", {})}

    def _extract_chatbot_info(self, config: RunnableConfig) -> Dict[str, Any]:
        """ì„¤ì •ì—ì„œ ì±—ë´‡ ì •ë³´ ì¶”ì¶œ"""
        try:
            if config and "configurable" in config:
                chatbot_data = config["configurable"].get("chatbot", {})
                if isinstance(chatbot_data, dict):
                    return chatbot_data
            return {}
        except Exception:
            return {}

    async def _load_chatmodel(
        self, chatbot_info: Dict[str, Any], config: RunnableConfig
    ) -> BaseChatModel:
        """LLM ëª¨ë¸ ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
        
        # LLM ì„œë¹„ìŠ¤ë¥¼ í†µí•´ì„œë§Œ ëª¨ë¸ ì ‘ê·¼ (ë°ì´í„° ì£¼ê¶Œ ì›ì¹™)
        from src.llm.service import get_llm_service
        
        llm_service = await get_llm_service()
        
        # ì±—ë´‡ ì •ë³´ì—ì„œ ëª¨ë¸ ì„¤ì • ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        model_overrides = {
            "streaming": True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        }
        
        # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œê°€ ìˆìœ¼ë©´ ì ìš©
        if model_overrides:
            config_obj = llm_service.update_config(**model_overrides)
            model = await llm_service.get_model(config_obj)
        else:
            # ê¸°ë³¸ ëª¨ë¸ì— ìŠ¤íŠ¸ë¦¬ë° ë°”ì¸ë”©
            model = llm_service.llm.bind(streaming=True)
        
        # ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        tools = await self._get_tool_list(config)
        
        # ë„êµ¬ê°€ ìˆìœ¼ë©´ ë°”ì¸ë”©
        if tools:
            model = model.bind_tools(tools)
        
        return model

    async def _get_tool_list(self, config: RunnableConfig) -> List[BaseTool]:
        """ì„¤ì •ì— ë”°ë¼ ì‚¬ìš©í•  ë„êµ¬ ëª©ë¡ ë°˜í™˜"""
        if not config or "configurable" not in config:
            return []
            
        configurable = config["configurable"]
        tools = []
        
        # ì„¤ì •ì— ë”°ë¼ ë„êµ¬ ì¶”ê°€
        for tool_name, tool in self.tools.items():
            if configurable.get(tool_name, False):
                tools.append(tool)
        
        return tools

    async def _create_prompt_template(
        self, chatbot_info: Dict[str, Any], config: RunnableConfig
    ) -> Any:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        if self.prompt_generator:
            return await self.prompt_generator.create_prompt_template(chatbot_info, config)
        else:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            from langchain_core.prompts import ChatPromptTemplate
            return ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
                ("human", "{messages}")
            ])

    def _manage_sql_data_state(self, state: LangGraphAgentState) -> LangGraphAgentState:
        """SQL ì¿¼ë¦¬ ê²°ê³¼ ë°ì´í„° ìƒíƒœ ê´€ë¦¬"""
        if "data" not in state:
            state["data"] = {}

        # ToolMessageì—ì„œ SQL ê²°ê³¼ ì¶”ì¶œí•˜ì—¬ ì €ì¥
        for message in state["messages"]:
            if isinstance(message, ToolMessage) and message.name == "sql_query":
                if hasattr(message, 'artifact') and message.artifact:
                    if message.artifact.get("status") == "success":
                        data_id = message.artifact.get("data_id", str(datetime.now().timestamp()))
                        state["data"][data_id] = {
                            "sql_query": message.artifact.get("sql_query", ""),
                            "data": message.artifact.get("data", ""),
                            "timestamp": datetime.now().isoformat(),
                        }

        return state

    async def _invoke_chain(
        self, chain: Runnable, messages: List[AnyMessage], config: RunnableConfig
    ) -> AIMessage:
        """ì²´ì¸ í˜¸ì¶œ"""
        try:
            return await chain.ainvoke({"messages": messages}, config=config)
        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬: {e}")
            raise Exception(f"LLM í˜¸ì¶œ ì¤‘ ì—ëŸ¬: {e}")

    def _add_message_metadata(
        self, message: AIMessage, existing_messages: List[BaseMessage]
    ) -> AIMessage:
        """ìƒˆ ë©”ì‹œì§€ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        # ê°€ì¥ ìµœê·¼ human ë©”ì‹œì§€ì˜ ID ì°¾ê¸°
        current_parent_id = None
        for msg in reversed(existing_messages):
            if hasattr(msg, 'type') and msg.type == "human":
                current_parent_id = getattr(msg, 'id', None)
                break

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if current_parent_id:
            if not hasattr(message, 'additional_kwargs') or message.additional_kwargs is None:
                message.additional_kwargs = {}
            
            message.additional_kwargs.update({
                "parent_id": current_parent_id,
                "timestamp": datetime.now().isoformat()
            })

        return message

    def _add_metadata_to_existing_messages(
        self, state: LangGraphAgentState, new_message: AIMessage
    ) -> LangGraphAgentState:
        """ê¸°ì¡´ ë©”ì‹œì§€ë“¤ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        current_parent_id = None
        if hasattr(new_message, 'additional_kwargs') and new_message.additional_kwargs:
            current_parent_id = new_message.additional_kwargs.get("parent_id")

        # ê¸°ì¡´ ë©”ì‹œì§€ë“¤ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for msg in state["messages"]:
            msg_type = getattr(msg, 'type', None)
            
            if msg_type == "human":
                # human ë©”ì‹œì§€ì—ëŠ” timestampë§Œ ì¶”ê°€
                if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                    msg.additional_kwargs = {}
                if "timestamp" not in msg.additional_kwargs:
                    msg.additional_kwargs["timestamp"] = datetime.now().isoformat()
            
            elif msg_type in ["ai", "tool"] and current_parent_id:
                # ai, tool ë©”ì‹œì§€ì—ëŠ” parent_idì™€ timestamp ì¶”ê°€
                if not hasattr(msg, 'additional_kwargs') or msg.additional_kwargs is None:
                    msg.additional_kwargs = {}
                
                if "parent_id" not in msg.additional_kwargs:
                    msg.additional_kwargs["parent_id"] = current_parent_id
                if "timestamp" not in msg.additional_kwargs:
                    msg.additional_kwargs["timestamp"] = datetime.now().isoformat()

        return state

    async def _accumulate_token_usage(self, chatbot_info: Dict[str, Any], message: AIMessage):
        """í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì """
        if not self.token_usage_service:
            return

        try:
            input_usage = 0
            output_usage = 0

            # ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            if hasattr(message, 'usage_metadata') and message.usage_metadata:
                input_usage = message.usage_metadata.get("input_tokens", 0)
                output_usage = message.usage_metadata.get("output_tokens", 0)
            elif hasattr(message, 'response_metadata') and message.response_metadata:
                input_usage = message.response_metadata.get("input_length", 0)
                output_usage = message.response_metadata.get("output_length", 0)

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ TokenUsage ê°ì²´ ìƒì„±)
            if input_usage > 0 or output_usage > 0:
                logger.info(f"í† í° ì‚¬ìš©ëŸ‰ - Input: {input_usage}, Output: {output_usage}")
                
        except Exception as e:
            logger.error(f"í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì  ì˜¤ë¥˜: {e}")


# ===== í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ë…¸ë“œë“¤ =====

async def agent_node(state: ReactAgentState) -> ReactAgentState:
    """
    ğŸ§  AGENT: LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜ ìµœì¢… ì‘ë‹µì„ ìƒì„±
    """
    try:
        container = await DIContainer.get_instance()
        llm = container.get("llm")
        
        # SQL Agentìš©ìœ¼ë¡œ ë„êµ¬ë¥¼ ë°”ì¸ë”©í•œ LLM ì‚¬ìš©
        llm_with_tools = llm.bind_tools(AVAILABLE_TOOLS)
        
        question = state["original_question"]
        iteration = state["iteration_count"]
        
        # DB ìŠ¤í‚¤ë§ˆ ì •ë³´
        schema_info = get_database_schema()
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
        tool_results = [msg for msg in state["messages"] if isinstance(msg, ToolMessage)]
        
        if not tool_results:
            # ì²« ë²ˆì§¸ ë°˜ë³µ: SQL ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰ ì§€ì‹œ
            prompt_text = get_react_agent_initial_prompt(
                question=question,
                schema_info=schema_info
            )
        else:
            # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°: ìµœì¢… ì‘ë‹µ ìƒì„±
            latest_tool_result = tool_results[-1].content
            prompt_text = get_react_agent_response_prompt(
                question=question,
                sql_result=latest_tool_result
            )
        
        prompt = ChatPromptTemplate.from_template(prompt_text)
        result = await llm_with_tools.ainvoke(prompt.format())
        
        # ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ ë¡œê¹…
        if hasattr(result, 'tool_calls') and result.tool_calls:
            logger.info(f"ğŸ§  ë„êµ¬ í˜¸ì¶œ ë°œê²¬: {len(result.tool_calls)}ê°œ")
        else:
            logger.info(f"ğŸ§  í…ìŠ¤íŠ¸ ì‘ë‹µ: {result.content[:100]}...")
        
        return {
            **state,
            "iteration_count": iteration + 1,
            "messages": state["messages"] + [result]
        }
        
    except Exception as e:
        logger.error(f"âŒ ì—ì´ì „íŠ¸ ë…¸ë“œ ì˜¤ë¥˜: {e}")
        error_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return {
            **state,
            "final_response": error_response,
            "is_complete": True,
            "messages": state["messages"] + [AIMessage(content=error_response)]
        }


async def finalize_node(state: ReactAgentState) -> ReactAgentState:
    """
    ğŸ“ FINALIZE: ìµœì¢… ì‘ë‹µ ì¶”ì¶œ ë° ì™„ë£Œ ì²˜ë¦¬
    """
    try:
        # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ë¥¼ ìµœì¢… ì‘ë‹µìœ¼ë¡œ ì‚¬ìš©
        ai_messages = [msg for msg in state["messages"] if isinstance(msg, AIMessage)]
        
        if ai_messages:
            final_response = ai_messages[-1].content
        else:
            final_response = "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        logger.info(f"ğŸ“ ìµœì¢… ì‘ë‹µ ì™„ë£Œ: {len(final_response)}ì")
        
        return {
            **state,
            "final_response": final_response,
            "is_complete": True
        }
        
    except Exception as e:
        logger.error(f"âŒ ìµœì¢…í™” ë…¸ë“œ ì˜¤ë¥˜: {e}")
        return {
            **state,
            "final_response": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "is_complete": True
        }


# ===== ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜ =====

def should_continue(state: ReactAgentState) -> Literal["tools", "finalize", "end"]:
    """
    ë‹¤ìŒ ë‹¨ê³„ ê²°ì •: ë„êµ¬ í˜¸ì¶œ â†’ ì™„ë£Œ â†’ ì¢…ë£Œ
    """
    
    # ì™„ë£Œ ì¡°ê±´ í™•ì¸
    if state["is_complete"]:
        return "end"
    
    # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼
    if state["iteration_count"] >= state["max_iterations"]:
        logger.info("ğŸ”„ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬, ì™„ë£Œ")
        return "finalize"
    
    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ í™•ì¸
    last_message = state["messages"][-1] if state["messages"] else None
    
    if isinstance(last_message, AIMessage):
        # AI ë©”ì‹œì§€ì— ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ”ì§€ í™•ì¸
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            logger.info("ğŸ› ï¸ ë„êµ¬ í˜¸ì¶œ ê°ì§€ë¨ â†’ tools")
            return "tools"
        else:
            # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì™„ë£Œ
            logger.info("ğŸ“ í…ìŠ¤íŠ¸ ì‘ë‹µ ê°ì§€ë¨ â†’ finalize")
            return "finalize"
    elif isinstance(last_message, ToolMessage):
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë‹¤ì‹œ ì—ì´ì „íŠ¸ë¡œ
        logger.info("ğŸ‘ï¸ ë„êµ¬ ê²°ê³¼ í™•ì¸ë¨ â†’ agent")
        return "agent"  # This should not happen in this routing function
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ì™„ë£Œ
    return "finalize"


# ===== í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ë“¤ =====

# ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
create_initial_state = create_react_initial_state

class AgentState(ReactAgentState):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ íƒ€ì… ë³„ì¹­"""
    pass

# ê¸°ì¡´ ë…¸ë“œë“¤ì„ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë§¤í•‘
reasoning_node = agent_node
action_node = agent_node  # ToolNodeê°€ ëŒ€ì‹  ì²˜ë¦¬
observation_node = agent_node  # ë„êµ¬ ì‹¤í–‰ í›„ ì—ì´ì „íŠ¸ê°€ ì²˜ë¦¬
generate_response_node = finalize_node

# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë¼ìš°íŒ… í•¨ìˆ˜
def should_continue_react(state: ReactAgentState) -> Literal["reasoning", "action", "observation", "finalize", "end"]:
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë¼ìš°íŒ… (ì‹¤ì œë¡œëŠ” ìƒˆë¡œìš´ êµ¬ì¡° ì‚¬ìš©)"""
    result = should_continue(state)
    
    # ìƒˆë¡œìš´ êµ¬ì¡°ë¥¼ ê¸°ì¡´ êµ¬ì¡°ë¡œ ë§¤í•‘
    if result == "tools":
        return "action"
    elif result == "finalize":
        return "finalize"
    elif result == "end":
        return "end"
    else:
        return "reasoning"